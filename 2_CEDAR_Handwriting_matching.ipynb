{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Data Processing\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "GSC_Features = pd.read_csv(r\"GSC-Dataset/GSC-Features-Data/GSC-Features.csv\")\n",
    "GSC_SamePairs = pd.read_csv(r\"GSC-Dataset/GSC-Features-Data/same_pairs.csv\")\n",
    "GSC_DiffPairs = pd.read_csv(r\"GSC-Dataset/GSC-Features-Data/diffn_pairs.csv\")\n",
    "\n",
    "\n",
    "print(GSC_Features.shape)\n",
    "print(GSC_Features.head())\n",
    "\n",
    "print(GSC_SamePairs.shape)\n",
    "print(GSC_SamePairs.head())\n",
    "\n",
    "print(GSC_DiffPairs.shape)\n",
    "print(GSC_DiffPairs.head())\n",
    "\n",
    "\n",
    "def genset(dataset_type, strtext,):\n",
    "    GSC_SameDiff = dataset_type.sample(n=10000).copy()\n",
    "    #GSC_SameDiff = pd.concat([GSC_SamePairs, GSC_DiffPairs], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    GSC_SameDiff.to_csv(\"GSC_SameDiff.csv\", sep=',')\n",
    "\n",
    "    print(\"Which GSC type :\"+strtext)\n",
    "    print(GSC_SameDiff.shape)\n",
    "    print(GSC_SameDiff.head())\n",
    "\n",
    "    GSC_SameDiff.rename(index=str, columns={\"img_id_A\": \"img_id\"}, inplace=True)\n",
    "\n",
    "    GSC_Dataset = pd.merge(GSC_SameDiff, GSC_Features,  how='inner', on=['img_id']).copy()\n",
    "\n",
    "    GSC_Dataset.rename(index=str, columns={\"img_id\": \"img_id_A\", \"img_id_B\": \"img_id\"}, inplace=True)\n",
    "\n",
    "    GSC_Dataset = pd.merge(GSC_Dataset, GSC_Features,  how='inner', on=['img_id']).copy()\n",
    "\n",
    "    GSC_Dataset.rename(index=str, columns={\"img_id\": \"img_id_B\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    GSC_SameDiffTrain = GSC_Dataset.sample(frac=0.7).copy()\n",
    "    GSC_SameDiffValTest = GSC_Dataset.loc[~GSC_Dataset.index.isin(GSC_SameDiffTrain.index)].copy()\n",
    "    GSC_SameDiffVal = GSC_SameDiffValTest.sample(n=1000).copy()\n",
    "    GSC_SameDiffTest = GSC_SameDiffValTest.loc[~GSC_SameDiffValTest.index.isin(GSC_SameDiffVal.index)].copy()\n",
    "\n",
    "    print(\"\\n Split up\")\n",
    "\n",
    "    print(GSC_SameDiffTrain.shape)\n",
    "    print(GSC_SameDiffTrain.head())\n",
    "    GSC_SameDiffTrain.to_csv(strtext+\"Train.csv\", sep=',', index=False)\n",
    "\n",
    "    print(GSC_SameDiffVal.shape)\n",
    "    print(GSC_SameDiffVal.head())\n",
    "    GSC_SameDiffVal.to_csv(strtext+\"Val.csv\", sep=',', index=False)\n",
    "\n",
    "\n",
    "    print(GSC_SameDiffTest.shape)\n",
    "    print(GSC_SameDiffTest.head())\n",
    "    GSC_SameDiffTest.to_csv(strtext+\"Test.csv\", sep=',', index=False)\n",
    "\n",
    "\n",
    "    print(GSC_Dataset.shape)\n",
    "    print(GSC_Dataset.head())\n",
    "    #GSC_Dataset.to_csv(\"GSC_DatasetTemp.csv\", sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''print(\"Testing the intersections for :\"+strtext)\n",
    "    GSC_SameDiffTestTraininter = GSC_SameDiffTrain.loc[GSC_SameDiffTrain.index.isin(GSC_SameDiffTest.index)].copy()\n",
    "    GSC_SameDiffValTraininter = GSC_SameDiffTrain.loc[GSC_SameDiffTrain.index.isin(GSC_SameDiffVal.index)].copy()\n",
    "    GSC_SameDiffValTestinter = GSC_SameDiffVal.loc[GSC_SameDiffVal.index.isin(GSC_SameDiffTest.index)].copy()\n",
    "    print(GSC_SameDiffTestTraininter.shape)\n",
    "    print(GSC_SameDiffTestTraininter.head())\n",
    "    print(GSC_SameDiffValTraininter.shape)\n",
    "    print(GSC_SameDiffValTraininter.head())\n",
    "    print(GSC_SameDiffValTestinter.shape)\n",
    "    print(GSC_SameDiffValTestinter.head()'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "genset(GSC_SamePairs, \"GSC_SamePairs\")\n",
    "genset(GSC_DiffPairs, \"GSC_DiffPairs\")\n",
    "\n",
    "\n",
    "\n",
    "GSC_SamePairsTrainset = pd.read_csv(r\"GSC_SamePairsTrain.csv\")\n",
    "GSC_DiffPairsTrainset = pd.read_csv(r\"GSC_DiffPairsTrain.csv\")\n",
    "\n",
    "GSC_SamePairsValset = pd.read_csv(r\"GSC_SamePairsVal.csv\")\n",
    "GSC_DiffPairsValset = pd.read_csv(r\"GSC_DiffPairsVal.csv\")\n",
    "\n",
    "GSC_SamePairsTestset = pd.read_csv(r\"GSC_SamePairsTest.csv\")\n",
    "GSC_DiffPairsTestset = pd.read_csv(r\"GSC_DiffPairsTest.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GSC_AllPairsTrain = pd.concat([GSC_SamePairsTrainset, GSC_DiffPairsTrainset], ignore_index=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "GSC_Raw_Concat = pd.read_csv(r\"GSC_Vivek_Raw_Concat.csv\", header=None)\n",
    "GSC_Raw_Subtract = pd.read_csv(r\"GSC_Vivek_Raw_Subtract.csv\", header=None)\n",
    "\n",
    "print(GSC_Raw_Concat.shape)\n",
    "print(GSC_Raw_Concat.head())\n",
    "\n",
    "print(GSC_Raw_Subtract.shape)\n",
    "print(GSC_Raw_Subtract.head())\n",
    "\n",
    "xdftemp = np.split(GSC_Raw_Concat, [1024], axis=1)\n",
    "xdf_concat = xdftemp[0].copy()\n",
    "xdf_concat.to_csv(\"GSC_Final_X_Raw_Concat.csv\", sep=',', index=False, header=False)\n",
    "tdf_concat = xdftemp[1].copy()\n",
    "tdf_concat.to_csv(\"GSC_Final_T_Raw_Concat.csv\", sep=',', index=False, header=False)\n",
    "\n",
    "print(xdf_concat.shape)\n",
    "print(xdf_concat.head())\n",
    "\n",
    "print(tdf_concat.shape)\n",
    "print(tdf_concat.head())\n",
    "\n",
    "\n",
    "\n",
    "xdftemp2 = np.split(GSC_Raw_Subtract, [512], axis=1)\n",
    "xdf_concat2 = xdftemp2[0].copy()\n",
    "xdf_concat2.to_csv(\"GSC_Final_X_Raw_Subtract.csv\", sep=',', index=False, header=False)\n",
    "tdf_concat2 = xdftemp2[1].copy()\n",
    "tdf_concat2.to_csv(\"GSC_Final_T_Raw_Subtract.csv\", sep=',', index=False, header=False)\n",
    "\n",
    "print(xdf_concat2.shape)\n",
    "print(xdf_concat2.head())\n",
    "\n",
    "print(tdf_concat2.shape)\n",
    "print(tdf_concat2.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "HOD_Features = pd.read_csv(r\"HumanObserved-Dataset/HumanObserved-Features-Data/HumanObserved-Features-Data.csv\")\n",
    "HOD_Features.drop(HOD_Features.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "HOD_SamePairs = pd.read_csv(r\"HumanObserved-Dataset/HumanObserved-Features-Data/same_pairs.csv\")\n",
    "HOD_DiffPairs = pd.read_csv(r\"HumanObserved-Dataset/HumanObserved-Features-Data/diffn_pairs.csv\")\n",
    "\n",
    "\n",
    "print(HOD_Features.shape)\n",
    "print(HOD_Features.head())\n",
    "\n",
    "print(HOD_SamePairs.shape)\n",
    "print(HOD_SamePairs.head())\n",
    "\n",
    "print(HOD_DiffPairs.shape)\n",
    "print(HOD_DiffPairs.head())\n",
    "\n",
    "\n",
    "def genset(dataset_type, strtext, aa=791):\n",
    "    HOD_SameDiff = dataset_type.sample(n=aa).copy()\n",
    "    #HOD_SameDiff = pd.concat([HOD_SamePairs, HOD_DiffPairs], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    HOD_SameDiff.to_csv(strtext+\".csv\", sep=',')\n",
    "\n",
    "    print(\"Which HOD type :\"+strtext)\n",
    "    print(\"How much :\"+str(aa))\n",
    "\n",
    "    print(HOD_SameDiff.shape)\n",
    "    print(HOD_SameDiff.head())\n",
    "\n",
    "    HOD_SameDiff.rename(index=str, columns={\"img_id_A\": \"img_id\"}, inplace=True)\n",
    "\n",
    "    HOD_Dataset = pd.merge(HOD_SameDiff, HOD_Features,  how='inner', on=['img_id']).copy()\n",
    "\n",
    "    HOD_Dataset.rename(index=str, columns={\"img_id\": \"img_id_A\", \"img_id_B\": \"img_id\"}, inplace=True)\n",
    "\n",
    "    HOD_Dataset = pd.merge(HOD_Dataset, HOD_Features,  how='inner', on=['img_id']).copy()\n",
    "\n",
    "    HOD_Dataset.rename(index=str, columns={\"img_id\": \"img_id_B\"}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    HOD_SameDiffTrain = HOD_Dataset.sample(frac=0.7).copy()\n",
    "    HOD_SameDiffValTest = HOD_Dataset.loc[~HOD_Dataset.index.isin(HOD_SameDiffTrain.index)].copy()\n",
    "    HOD_SameDiffVal = HOD_SameDiffValTest.sample(frac=0.66).copy()\n",
    "    HOD_SameDiffTest = HOD_SameDiffValTest.loc[~HOD_SameDiffValTest.index.isin(HOD_SameDiffVal.index)].copy()\n",
    "\n",
    "    print(\"\\n Split up\")\n",
    "\n",
    "    print(HOD_SameDiffTrain.shape)\n",
    "    print(HOD_SameDiffTrain.head())\n",
    "    HOD_SameDiffTrain.to_csv(strtext+\"Train.csv\", sep=',', index=False)\n",
    "\n",
    "    print(HOD_SameDiffVal.shape)\n",
    "    print(HOD_SameDiffVal.head())\n",
    "    HOD_SameDiffVal.to_csv(strtext+\"Val.csv\", sep=',', index=False)\n",
    "\n",
    "\n",
    "    print(HOD_SameDiffTest.shape)\n",
    "    print(HOD_SameDiffTest.head())\n",
    "    HOD_SameDiffTest.to_csv(strtext+\"Test.csv\", sep=',', index=False)\n",
    "\n",
    "\n",
    "    print(HOD_Dataset.shape)\n",
    "    print(HOD_Dataset.head())\n",
    "    #HOD_Dataset.to_csv(\"HOD_DatasetTemp.csv\", sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''print(\"Testing the intersections for :\"+strtext)\n",
    "    HOD_SameDiffTestTraininter = HOD_SameDiffTrain.loc[HOD_SameDiffTrain.index.isin(HOD_SameDiffTest.index)].copy()\n",
    "    HOD_SameDiffValTraininter = HOD_SameDiffTrain.loc[HOD_SameDiffTrain.index.isin(HOD_SameDiffVal.index)].copy()\n",
    "    HOD_SameDiffValTestinter = HOD_SameDiffVal.loc[HOD_SameDiffVal.index.isin(HOD_SameDiffTest.index)].copy()\n",
    "    print(HOD_SameDiffTestTraininter.shape)\n",
    "    print(HOD_SameDiffTestTraininter.head())\n",
    "    print(HOD_SameDiffValTraininter.shape)\n",
    "    print(HOD_SameDiffValTraininter.head())\n",
    "    print(HOD_SameDiffValTestinter.shape)\n",
    "    print(HOD_SameDiffValTestinter.head()'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "genset(HOD_SamePairs, \"HOD_SamePairs\")\n",
    "genset(HOD_DiffPairs, \"HOD_DiffPairs\", 1845)\n",
    "\n",
    "\n",
    "\n",
    "'''HOD_SamePairsTrainset = pd.read_csv(r\"HOD_SamePairsTrain.csv\")\n",
    "HOD_DiffPairsTrainset = pd.read_csv(r\"HOD_DiffPairsTrain.csv\")\n",
    "\n",
    "HOD_SamePairsValset = pd.read_csv(r\"HOD_SamePairsVal.csv\")\n",
    "HOD_DiffPairsValset = pd.read_csv(r\"HOD_DiffPairsVal.csv\")\n",
    "\n",
    "HOD_SamePairsTestset = pd.read_csv(r\"HOD_SamePairsTest.csv\")\n",
    "HOD_DiffPairsTestset = pd.read_csv(r\"HOD_DiffPairsTest.csv\")'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#HOD_AllPairsTrain = pd.concat([HOD_SamePairsTrainset, HOD_DiffPairsTrainset], ignore_index=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "#from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "#import matplotlib.pyplot\n",
    "#from matplotlib import pyplot as plt\n",
    "'''df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6],\n",
    "                   'b': [4, 6, 4, 7, 15, 18],\n",
    "                   'c': [1, 4, 9, 16, 25, 36],\n",
    "                   'd': [6, 8, 9, 26, 55, 66]})\n",
    "\n",
    "#print(df[0].values)\n",
    "print(df)\n",
    "#print(col_list[0])\n",
    "#print(df[col_list[0]].values)\n",
    "#print(df)\n",
    "col_list = df.columns\n",
    "\n",
    "for i in range(0,2):\n",
    "    df.iloc[:, [i]] =abs(df.iloc[:, [i+2]].values-df.iloc[:, [i]].values)\n",
    "\n",
    "print(df.iloc[:, [1]])\n",
    "print(df.shape)\n",
    "df.drop(df.columns[0:2], axis=1, inplace=True)\n",
    "print(df)\n",
    "\n",
    "#   2\t1\t1             1\t   1\t2\t              2 \t0\t0           \t2247,2245'''\n",
    "\n",
    "\n",
    "\n",
    "'''def GetTargetVector(filePath):\n",
    "    t = []\n",
    "    with open(filePath, 'rU') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            t.append(int(row[0]))\n",
    "    #print(\"Raw Training Generated..\")\n",
    "    return t\n",
    "\n",
    "\n",
    "def GenerateRawData(filePath, IsSynthetic):\n",
    "    dataMatrix = []\n",
    "    with open(filePath, 'rU') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            dataRow = []\n",
    "            for column in row:\n",
    "                dataRow.append(float(column))\n",
    "            dataMatrix.append(dataRow)\n",
    "\n",
    "    if IsSynthetic == False:\n",
    "        dataMatrix = np.delete(dataMatrix, [5, 6, 7, 8, 9], axis=1)\n",
    "    dataMatrix = np.transpose(dataMatrix)\n",
    "    # print (\"Data Matrix Generated..\")\n",
    "    return dataMatrix\n",
    "IsSynthetic=True\n",
    "RawData   = GenerateRawData('/Users/vivad/Desktop/ML/Proj1-2python-code/Querylevelnorm_X.csv',IsSynthetic)\n",
    "RawData1 = RawData\n",
    "print(RawData.shape)\n",
    "print(RawData)\n",
    "RawData = np.concatenate((RawData, RawData1), axis=1)\n",
    "print(RawData)\n",
    "\n",
    "print(RawData.shape)\n",
    "csvdf =pd.read_csv(\"/Users/vivad/Desktop/ML/Proj1-2python-code/Querylevelnorm_X.csv\")\n",
    "\n",
    "print(csvdf.shape)\n",
    "print(csvdf.head())\n",
    "\n",
    "\n",
    "\n",
    "#RawTarget = GetTargetVector('HOD_SamePairsTrain.csv')\n",
    "\n",
    "#RawData   = GenerateRawData('/Users/vivad/Desktop/ML/Proj1-2python-code/Querylevelnorm_.csv',IsSynthetic=False)'''\n",
    "\n",
    "\n",
    "\n",
    "HOD_SamePairsTrainset = pd.read_csv(r\"HOD_SamePairsTrain.csv\")\n",
    "HOD_DiffPairsTrainset = pd.read_csv(r\"HOD_DiffPairsTrain.csv\")\n",
    "\n",
    "HOD_SamePairsValset = pd.read_csv(r\"HOD_SamePairsVal.csv\")\n",
    "HOD_DiffPairsValset = pd.read_csv(r\"HOD_DiffPairsVal.csv\")\n",
    "\n",
    "HOD_SamePairsTestset = pd.read_csv(r\"HOD_SamePairsTest.csv\")\n",
    "HOD_DiffPairsTestset = pd.read_csv(r\"HOD_DiffPairsTest.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HOD_AllPairsTrain = pd.concat([HOD_SamePairsTrainset, HOD_DiffPairsTrainset], ignore_index=True).copy()\n",
    "print(\"HOD_AllPairsTrain\")\n",
    "print(HOD_AllPairsTrain.shape)\n",
    "print(HOD_AllPairsTrain.head())\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "HOD_AllPairsVal = pd.concat([HOD_SamePairsValset, HOD_DiffPairsValset], ignore_index=True).copy()\n",
    "print(\"HOD_AllPairsVal\")\n",
    "print(HOD_AllPairsVal.shape)\n",
    "print(HOD_AllPairsVal.head())\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "HOD_AllPairsTest = pd.concat([HOD_SamePairsTestset, HOD_DiffPairsTestset], ignore_index=True).copy()\n",
    "print(\"HOD_AllPairsTest\")\n",
    "print(HOD_AllPairsTest.shape)\n",
    "print(HOD_AllPairsTest.head())\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "HOD_AllPairsTrainValTest = pd.concat([HOD_AllPairsTrain, HOD_AllPairsVal, HOD_AllPairsTest ], ignore_index=True).copy()\n",
    "print(\"HOD_AllPairsTrainValTest\")\n",
    "print(HOD_AllPairsTrainValTest.shape)\n",
    "print(HOD_AllPairsTrainValTest.head())\n",
    "\n",
    "print()\n",
    "HOD_AllPairsTrainValTestCopy = HOD_AllPairsTrainValTest.copy()\n",
    "HOD_AllPairsTrainValTest.insert(21, 'target_val', HOD_AllPairsTrainValTest['target'])\n",
    "HOD_AllPairsTrainValTest = HOD_AllPairsTrainValTest.drop(HOD_AllPairsTrainValTest.columns[[2]], axis=1)\n",
    "\n",
    "print(HOD_AllPairsTrainValTest.shape)\n",
    "print(HOD_AllPairsTrainValTest.head())\n",
    "col_list = list(HOD_AllPairsTrainValTest)\n",
    "\n",
    "print(col_list)\n",
    "\n",
    "HOD_AllPairsTrainValTest.to_csv(\"HOD_Vivek_RawDatasetConcat.csv\", sep=',', index=False)\n",
    "HOD_AllPairsTrainValTest.to_csv(\"HOD_Vivek_RawDatasetTake.csv\", sep=',', index=False)\n",
    "\n",
    "HOD_AllPairsTrainValTestSubtract = HOD_AllPairsTrainValTest.copy()\n",
    "for i in range(2,11):\n",
    "    #mergedStuff_subtract[col_list[i]] =abs(mergedStuff_dupe[col_list[i+9]].values-mergedStuff_dupe[col_list[i]].values)\n",
    "    HOD_AllPairsTrainValTestSubtract.iloc[:, [i]] =abs(HOD_AllPairsTrainValTestSubtract.iloc[:, [i+9]].values-HOD_AllPairsTrainValTestSubtract.iloc[:, [i]].values)\n",
    "\n",
    "\n",
    "print(\" HOD Subtract\")\n",
    "HOD_AllPairsTrainValTestSubtract.drop(HOD_AllPairsTrainValTestSubtract.columns[11:20], axis=1, inplace=True)\n",
    "print(HOD_AllPairsTrainValTestSubtract.shape)\n",
    "print(HOD_AllPairsTrainValTestSubtract.head())\n",
    "col_list = list(HOD_AllPairsTrainValTestSubtract)\n",
    "\n",
    "print(col_list)\n",
    "HOD_AllPairsTrainValTestSubtract.to_csv(\"HOD_Vivek_RawDatasetSubtract.csv\", sep=',', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Raw concat\n",
    "HOD_AllPairsTrainValTest.drop(HOD_AllPairsTrainValTest.columns[0:2], axis=1, inplace=True)\n",
    "\n",
    "HOD_AllPairsTrainValTest.to_csv(\"HOD_Vivek_Raw_Concat.csv\", sep=',', index=False, header=False)#USe this\n",
    "\n",
    "\n",
    "\n",
    "HOD_AllPairsTrainValTestSubtract.drop(HOD_AllPairsTrainValTestSubtract.columns[0:2], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "HOD_AllPairsTrainValTestSubtract.to_csv(\"HOD_Vivek_Raw_Subtract.csv\", sep=',', index=False, header=False)#Use this\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "HOD_Raw_Concat = pd.read_csv(r\"HOD_Vivek_Raw_Concat.csv\", header=None)\n",
    "HOD_Raw_Subtract = pd.read_csv(r\"HOD_Vivek_Raw_Subtract.csv\", header=None)\n",
    "\n",
    "print(HOD_Raw_Concat.shape)\n",
    "print(HOD_Raw_Concat.head())\n",
    "\n",
    "print(HOD_Raw_Subtract.shape)\n",
    "print(HOD_Raw_Subtract.head())\n",
    "\n",
    "xdftemp = np.split(HOD_Raw_Concat, [18], axis=1)\n",
    "xdf_concat = xdftemp[0].copy()\n",
    "xdf_concat.to_csv(\"HOD_Final_X_Raw_Concat.csv\", sep=',', index=False, header=False)\n",
    "tdf_concat = xdftemp[1].copy()\n",
    "tdf_concat.to_csv(\"HOD_Final_T_Raw_Concat.csv\", sep=',', index=False, header=False)\n",
    "\n",
    "print(xdf_concat.shape)\n",
    "print(xdf_concat.head())\n",
    "\n",
    "print(tdf_concat.shape)\n",
    "print(tdf_concat.head())\n",
    "\n",
    "\n",
    "\n",
    "xdftemp2 = np.split(HOD_Raw_Subtract, [9], axis=1)\n",
    "xdf_concat2 = xdftemp2[0].copy()\n",
    "xdf_concat2.to_csv(\"HOD_Final_X_Raw_Subtract.csv\", sep=',', index=False, header=False)\n",
    "tdf_concat2 = xdftemp2[1].copy()\n",
    "tdf_concat2.to_csv(\"HOD_Final_T_Raw_Subtract.csv\", sep=',', index=False, header=False)\n",
    "\n",
    "print(xdf_concat2.shape)\n",
    "print(xdf_concat2.head())\n",
    "\n",
    "print(tdf_concat2.shape)\n",
    "print(tdf_concat2.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0.0\n",
    "maxIter = 0\n",
    "# Varied from 0.1 to 1, 0.1 steps\n",
    "C_Lambda = 0.3\n",
    "TrainingPercent = 70\n",
    "ValidationPercent = 10\n",
    "TestPercent = 20\n",
    "M = 17\n",
    "PHI = []\n",
    "IsSynthetic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This fucntion is used to retrieve the target vector from the Querylevelnorm_t\n",
    "# CSV file and append it to t [], which forms the RawTarget\n",
    "def GetTargetVector(filePath):\n",
    "    t = []\n",
    "    with open(filePath, 'rU') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:  \n",
    "            t.append(int(row[0]))\n",
    "    #print(\"Raw Training Generated..\")\n",
    "    return t\n",
    "# This function is used to get the Raw Data Matrix from the input csv file\n",
    "# by passing the file's path, which is used to train the model\n",
    "def GenerateRawData(filePath, IsSynthetic):    \n",
    "    dataMatrix = [] \n",
    "    with open(filePath, 'rU') as fi:\n",
    "        reader = csv.reader(fi)\n",
    "        for row in reader:\n",
    "            dataRow = []\n",
    "            for column in row:\n",
    "                dataRow.append(float(column))\n",
    "            dataMatrix.append(dataRow)   \n",
    "    \n",
    "    if IsSynthetic == False :\n",
    "        dataMatrix = np.delete(dataMatrix, [5,6,7,8,9], axis=1)\n",
    "    dataMatrix = np.transpose(dataMatrix)     \n",
    "    #print (\"Data Matrix Generated..\")\n",
    "    return dataMatrix\n",
    "# This function is used to obtained the set of Target Variables or\n",
    "# Response values for the purpose training, based on 80-10-10 split, with 80% for training\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 70):\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*TrainingPercent*0.01))\n",
    "    t= rawTraining[:TrainingLen]\n",
    "    print(\"TrainingLen\"+str(TrainingLen))\n",
    "    print(\"RawTrainingLen\"+str( len(rawTraining)))\n",
    "\n",
    "\n",
    "    print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "# This function is used to obtain the  matrix of set of data values\n",
    "# for the purpose training\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 70):\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "# This function is used to obtain the  matrix of set of data values\n",
    "# for the purpose validation\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    print(\"Vend\"+str(V_End))\n",
    "    print(\"TrainingPerc\"+str(TrainingCount+1))\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "# This function is used to obtain the set of Target Variables or\n",
    "# Response values for the purpose validation\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "\n",
    "# Function which is used to produce the covariance values as a  diagonal matrix\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic):\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    DataT       = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct))\n",
    "    #####\n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]+0.2\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    return BigSigma\n",
    "\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow)\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  \n",
    "    L = np.dot(R,T)\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv))\n",
    "    return phi_x\n",
    "# Function that is invoked to create the matrix to get function values of the input features\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 70):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*TrainingPercent*0.01) \n",
    "    #    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    print(\"1.  \"+str(len(DataT)*TrainingPercent*0.01))\n",
    "    #print(str())\n",
    "\n",
    "\n",
    "\n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    print(\"Len Mumatrix\"+str(len(MuMatrix)))\n",
    "    print(\"int(TrainingLen)\"+str(int(TrainingLen)))\n",
    "\n",
    "\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI\n",
    "\n",
    "# Function to get the weight matrix by using the closed-form approach \n",
    "def GetWeightsClosedForm(PHI, T, Lambda):\n",
    "    Lambda_I = np.identity(len(PHI[0]))\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)\n",
    "    W           = np.dot(INTER, T)\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "'''def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma)\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI'''\n",
    "\n",
    "def GetValTest(VAL_PHI,W):\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "# Function to generate the Error Root Mean Square\n",
    "# with difference between  Predicted and Observed value as the the basis\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: 'U' mode is deprecated\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: 'U' mode is deprecated\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 20000)\n",
      "20000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"# For subtracted dataset\\nRawTarget = GetTargetVector('UsableDataset/GSC_Final_T_Raw_Subtract.csv')\\nRawData   = GenerateRawData('UsableDataset/GSC_Final_X_Raw_Subtract.csv',True)\\n\\n\\n\\nprint(RawData.shape)\\n#print(RawData.head())\\n\\n\\nprint(len(RawTarget))\\n#print(RawTarget.head())\""
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For concatenated dataset\n",
    "\n",
    "\n",
    "#GSCVivek/3/GSC_Final_T_Raw_Concat.csv\n",
    "#GSCVivek/3/GSC_Final_X_Raw_Concat.csv\n",
    "#GSCVivek/3/GSC_Final_T_Raw_Subtract.csv\n",
    "#GSCVivek/3/GSC_Final_X_Raw_Subtract.csv\n",
    "#RawTarget = GetTargetVector('UsableDataset/GSC_Final_T_Raw_Concat.csv')\n",
    "#RawData   = GenerateRawData('UsableDataset/GSC_Final_X_Raw_Concat.csv',True)\n",
    "RawTarget = GetTargetVector('GSCVivek/3/GSC_Final_T_Raw_Concat.csv')\n",
    "RawData   = GenerateRawData('GSCVivek/3/GSC_Final_X_Raw_Concat.csv',True)\n",
    "#/Users/vivad/Downloads/final-gsc.csv\n",
    "\n",
    "print(RawData.shape)\n",
    "#print(RawData.head())\n",
    "\n",
    "\n",
    "print(len(RawTarget))\n",
    "#print(RawTarget.head())\n",
    "\n",
    "\"\"\"# For subtracted dataset\n",
    "RawTarget = GetTargetVector('UsableDataset/GSC_Final_T_Raw_Subtract.csv')\n",
    "RawData   = GenerateRawData('UsableDataset/GSC_Final_X_Raw_Subtract.csv',True)\n",
    "\n",
    "\n",
    "\n",
    "print(RawData.shape)\n",
    "#print(RawData.head())\n",
    "\n",
    "\n",
    "print(len(RawTarget))\n",
    "#print(RawTarget.head())\"\"\"\n",
    "    \n",
    "#RawInputConcat()\n",
    "#RawInputSubtract()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingLen14000\n",
      "RawTrainingLen20000\n",
      "70% Training Target Generated..\n",
      "70% Training Data Generated..\n",
      "(14000,)\n",
      "14000\n",
      "(1024, 14000)\n"
     ]
    }
   ],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget,TrainingPercent))\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)\n",
    "print(TrainingTarget.shape)\n",
    "print(len(TrainingTarget))\n",
    "\n",
    "\n",
    "print(TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vend16000\n",
      "TrainingPerc14001\n",
      "(1999,)\n",
      "(1024, 1999)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget))))\n",
    "ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget)))\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vend19999\n",
      "TrainingPerc16000\n",
      "3999\n",
      "(3999,)\n",
      "(1024, 3999)\n"
     ]
    }
   ],
   "source": [
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n",
    "print(len(TestDataAct))\n",
    "print(TestDataAct.shape)\n",
    "\n",
    "\n",
    "print(TestData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Form Solution [Finding Weights using Moore- Penrose pseudo- Inverse Matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  14000.0\n",
      "Len Mumatrix17\n",
      "int(TrainingLen)14000\n",
      "1.  3999.0\n",
      "Len Mumatrix17\n",
      "int(TrainingLen)3999\n",
      "1.  1999.0\n",
      "Len Mumatrix17\n",
      "int(TrainingLen)1999\n"
     ]
    }
   ],
   "source": [
    "ErmsArr = []\n",
    "AccuracyArr = []\n",
    "# This is used to create the clustering algorithms using K-Means clustering\n",
    "# where M is the number of clusters\n",
    "# Clustering is done to convert large dataset in to small clusters by finding the centroid\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData))\n",
    "Mu = kmeans.cluster_centers_\n",
    "\n",
    "BigSigma     = GenerateBigSigma(RawData, Mu, TrainingPercent,IsSynthetic)\n",
    "TRAINING_PHI = GetPhiMatrix(RawData, Mu, BigSigma, TrainingPercent)\n",
    "W            = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget,(C_Lambda)) \n",
    "TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100) \n",
    "VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 1024)\n",
      "(1024, 1024)\n",
      "(14000, 17)\n",
      "(17,)\n",
      "(1999, 17)\n",
      "(3999, 17)\n"
     ]
    }
   ],
   "source": [
    "print(Mu.shape)\n",
    "print(BigSigma.shape)\n",
    "print(TRAINING_PHI.shape)\n",
    "print(W.shape)\n",
    "print(VAL_PHI.shape)\n",
    "print(TEST_PHI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSC Subtraction results for M=17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent solution for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "--------------Please Wait for 2 mins!----------------\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print ('----------------------------------------------------')\n",
    "print ('--------------Please Wait for 2 mins!----------------')\n",
    "print ('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W_Now        = np.dot(220, W)\n",
    "La           = 2\n",
    "learningRate = 0.1\n",
    "epochs = 300\n",
    "L_Erms_Val   = []\n",
    "L_Erms_TR    = []\n",
    "L_Erms_Test  = []\n",
    "W_Mat        = []\n",
    "I_list = []\n",
    "Erms_list = []\n",
    "L_Erms_Acc = []\n",
    "\n",
    "\n",
    "# Iterates over the phi matrix rows for specific epochs to converge towards the local minima value\n",
    "for i in range(0, epochs):\n",
    "    #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "    Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "    La_Delta_E_W  = np.dot(La,W_Now)\n",
    "    Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "    W_T_Next      = W_Now + Delta_W\n",
    "    W_Now         = W_T_Next\n",
    "    \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "    Erms_Test = GetErms(TEST_OUT,TestDataAct)\n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "    L_Erms_Acc.append(float(Erms_Test.split(',')[0]))\n",
    "\n",
    "\n",
    "    I_list.append(i)\n",
    "    \n",
    "plt.plot(I_list, L_Erms_Test, linewidth = 0.5)\n",
    "plt.xlabel('Epoch Count')\n",
    "plt.ylabel('E-RMS value for Testing')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Linear Regression--------------------\n",
      "----------Gradient Descent Solution--------------------\n",
      "----------By Vivek Adithya--------------------\n",
      "----------UB# 50290568--------------------\n",
      "E_rms Training   = 0.46492\n",
      "E_rms Validation = 0.46443\n",
      "E_rms Testing    = 0.46957\n",
      "Testing Accuracy = 63.94099\n",
      "Cluster Count =17\n",
      "Iterations =300\n",
      "Lambda =2\n",
      "Learning Rate =0.1\n"
     ]
    }
   ],
   "source": [
    "print ('----------Linear Regression--------------------')\n",
    "\n",
    "\n",
    "print ('----------Gradient Descent Solution--------------------')\n",
    "print ('----------By Vivek Adithya--------------------')\n",
    "print ('----------UB# 50290568--------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "print (\"Testing Accuracy = \" + str(np.around(max(L_Erms_Acc),5)))\n",
    "print(\"Cluster Count =\"+str(M)+\"\\n\"+\"Iterations =\"+str(epochs))\n",
    "print(\"Lambda =\"+str(La)+\"\\n\"+\"Learning Rate =\"+str(learningRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M=17, LinReg GSC Subtract results above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "####################\n",
    "#############\n",
    "################\n",
    "#\n",
    "#     Logistic Regression\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "#/Users/vivad/PycharmProjects/ML_Project2/GSCVivek/3/GSC_Final_T_Raw_Concat.csv\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/GSCVivek/3/GSC_Final_X_Raw_Concat.csv\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/GSCVivek/3/GSC_Final_T_Raw_Subtract.csv\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/GSCVivek/3/GSC_Final_X_Raw_Subtract.csv\n",
    "\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/HODVivek/3/HOD_Final_T_Raw_Concat.csv\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/HODVivek/3/HOD_Final_X_Raw_Concat.csv\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/HODVivek/3/HOD_Final_T_Raw_Subtract.csv\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/HODVivek/3/HOD_Final_X_Raw_Subtract.csv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "XXX = pd.read_csv(r\"HODVivek/3/HOD_Final_X_Raw_Concat.csv\", header=None, index_col=False)\n",
    "XT = XXX[:math.ceil(len(XXX)*0.8) ]\n",
    "XX = XT.as_matrix()\n",
    "XTest = XXX[math.ceil(len(XXX)*0.8):len(XXX) ]\n",
    "XTT = XTest.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "YYY = pd.read_csv(r\"HODVivek/3/HOD_Final_T_Raw_Concat.csv\", header=None, index_col=False)\n",
    "YTemp = YYY[:math.ceil(len(YYY)*0.8) ]\n",
    "YYYY = YTemp.as_matrix()\n",
    "YTest = YYY[math.ceil(len(YYY)*0.8):len(YYY) ]\n",
    "YT1 = YTest.as_matrix()\n",
    "YY = YYYY.reshape(-1)\n",
    "YTT = YT1.reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = XX.copy()\n",
    "y = YY.copy()\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            loss = self.__loss(h, y)\n",
    "                \n",
    "            if(self.verbose ==True and i % 10000 == 0):\n",
    "                print(f'loss: {loss} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "####\n",
      "Please wait for 5 mins or change num_iter to lesser value\n",
      "Note: 1000 iters takes approx 25 seconds \n",
      "####\n"
     ]
    }
   ],
   "source": [
    "print(\"####\")\n",
    "print(\"####\")\n",
    "print(\"Please wait for 5 mins or change num_iter to lesser value\")\n",
    "print(\"Note: 1000 iters takes approx 25 seconds \")\n",
    "print(\"####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "num_iter=1000000\n",
    "model = LogisticRegression(lr, num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 36s, sys: 4.33 s, total: 9min 40s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%time model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Prediction Accuracy: 0.8462998102466793\n",
      "Learning Rate:0.01\n",
      "Iterations:1000000\n"
     ]
    }
   ],
   "source": [
    "# Prediction for logistic regression based on test set labels\n",
    "preds = model.predict(XTT)\n",
    "print(\"Logistic Regression Prediction Accuracy: \"+str((preds == YTT).mean()))\n",
    "print(\"Learning Rate:\"+str(lr))\n",
    "print(\"Iterations:\"+str(num_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "###############################################\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "## NEURAL NETWORKS\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(dataset, k=1):\n",
    "    \n",
    "    # We need to process to obtain the input column data and\n",
    "    # the label column data, and store it separately\n",
    "    #data   = dataset['input'].values\n",
    "    #labels = dataset['label'].values\n",
    "    \n",
    "    data   = dataset.iloc[:18000,2:1026]\n",
    "    labels = dataset.iloc[:18000,1026]\n",
    "    if(k==2):\n",
    "        return dataset.iloc[18000:,2:]\n",
    "        \n",
    "    '''print(data)\n",
    "    print(data.shape)\n",
    "    print(data.head())\n",
    "    print(type(data))\n",
    "    \n",
    "    print(labels)\n",
    "    print(labels.shape)\n",
    "    print(labels.head())\n",
    "    print(type(data))'''\n",
    "    \n",
    "    print()\n",
    "    # Processing the input data using the specified \n",
    "    # encodeData method where each individual element is processed\n",
    "    processedData  = data.as_matrix()\n",
    "    \n",
    "    # Processing the labels to encode them to specific\n",
    "    # categorical target values like \n",
    "    # FizzBuzz = 3, Fizz = 1, Buzz = 2, Other = 0\n",
    "    processedLabel = encodeLabel(labels)\n",
    "    \n",
    "    return processedData, processedLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def encodeLabel(labels):\n",
    "    \n",
    "    # Processing the labels corresponding to inputs and \n",
    "    # encoding them to respective categorical values like \n",
    "    # FizzBuzz = 3, Fizz = 1, Buzz = 2, other=0\n",
    "    \n",
    "    processedLabel = []\n",
    "    \n",
    "    for labelInstance in labels:\n",
    "        if(labelInstance == 0):\n",
    "            # For FizzBuzz label\n",
    "            processedLabel.append([0])\n",
    "        elif(labelInstance == 1):\n",
    "            # For Fizz label\n",
    "            processedLabel.append([1])\n",
    "\n",
    "        else:\n",
    "            # For any other label case\n",
    "            print(\"Check encode labels\")\n",
    "    \n",
    "    # Converting each of the class vector(integers) to \n",
    "    # its binary representation for 4 different class types.\n",
    "    return np_utils.to_categorical(np.array(processedLabel),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input_size = 1024\n",
    "drop_out = 0.1\n",
    "first_dense_layer_nodes  = 200\n",
    "second_dense_layer_nodes = 2\n",
    "\n",
    "def get_model():\n",
    "    # Why do we need a model?\n",
    "    # Models are artifacts that are created in due course\n",
    "    # by machine learning algorithms.\n",
    "    # Models are used for training the system to predict \n",
    "    # or anticipate for other unknown values. \n",
    "    \n",
    "    # Why use sequential model with layers?\n",
    "    # In Sequential model, each layer works in tandem with subsequent \n",
    "    # layers just before or after the layer itsel. Compared to Functional\n",
    "    # model, Sequential model is straightforward. \n",
    "    # Sequential model is used here because there is no need for sharing\n",
    "    # of layers and the systme does not have multiple inputs and outputs. \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Why use Dense layer and then activation?\n",
    "    # A dense layer is a typical densely connected \n",
    "    # Neural Network layer. \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size, activation='linear'))\n",
    "    \n",
    "    # Activation Function is passed element-wise to every output. \n",
    "    # Activation applies an activation function to the output.\n",
    "    # Here Leaky Relu activation is used, with alpha=0.001 being the \n",
    "    # small negative slope \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Optimizer models in comments\n",
    "    \n",
    "    # model.add(LeakyReLU(alpha=.001))\n",
    "    \n",
    "    # model.add(Activation('relu'))\n",
    "    \n",
    "    # model.add(Activation('sigmoid'))\n",
    "\n",
    "    # model.add(Activation('tanh'))\n",
    "    \n",
    "    model.add(LeakyReLU(alpha=.001))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # Why dropout?\n",
    "    # Dropout is a value that denotes the fraction of input values that is set to 0\n",
    "    # or ignored for dealing with the problem overfitting.\n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    # Add second dense layer of nodes containing the 4 target nodes\n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    \n",
    "    # Softmax is used for multiple classification. \n",
    "    # Softmax function computes the probabilty of each \n",
    "    # probable output class, assigns a value between 0 to 1, \n",
    "    # where the sum of probabilties of every class results in value one.\n",
    "    \n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Summarization of the model is displayed.\n",
    "    model.summary()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compile method is used to configure the model training \n",
    "    # or the learning process and takes 3 parameters - the optimizer,\n",
    "    # the loss function and a list of metrics to be observed.\n",
    "    # Categorical_crossentropy is the loss function, which forms\n",
    "    # the main objective that the model looks to minimize.\n",
    "    # Categorical_crossentropy is used here because it deals with\n",
    "    # multi class classification problem as this one, where the target range \n",
    "    # is 4 different classes : Fizz, Buzz, FizzBuzz and other respectively.\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 200)               205000    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 402       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 205,402\n",
      "Trainable params: 205,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14400 samples, validate on 3600 samples\n",
      "Epoch 1/30\n",
      "14400/14400 [==============================] - 4s 273us/step - loss: 0.7509 - acc: 0.5352 - val_loss: 0.6994 - val_acc: 0.5228\n",
      "Epoch 2/30\n",
      "14400/14400 [==============================] - 3s 216us/step - loss: 0.6334 - acc: 0.6378 - val_loss: 0.6601 - val_acc: 0.5886\n",
      "Epoch 3/30\n",
      "14400/14400 [==============================] - 3s 232us/step - loss: 0.5562 - acc: 0.7217 - val_loss: 0.5293 - val_acc: 0.7578\n",
      "Epoch 4/30\n",
      "14400/14400 [==============================] - 3s 231us/step - loss: 0.4939 - acc: 0.7653 - val_loss: 0.4663 - val_acc: 0.8078\n",
      "Epoch 5/30\n",
      "14400/14400 [==============================] - 3s 233us/step - loss: 0.4322 - acc: 0.8077 - val_loss: 0.4182 - val_acc: 0.8231\n",
      "Epoch 6/30\n",
      "14400/14400 [==============================] - 3s 228us/step - loss: 0.3848 - acc: 0.8374 - val_loss: 0.4417 - val_acc: 0.8025\n",
      "Epoch 7/30\n",
      "14400/14400 [==============================] - 3s 238us/step - loss: 0.3434 - acc: 0.8628 - val_loss: 0.4175 - val_acc: 0.8125\n",
      "Epoch 8/30\n",
      "14400/14400 [==============================] - 3s 232us/step - loss: 0.3099 - acc: 0.8771 - val_loss: 0.3637 - val_acc: 0.8453\n",
      "Epoch 9/30\n",
      "14400/14400 [==============================] - 4s 246us/step - loss: 0.2814 - acc: 0.8895 - val_loss: 0.3759 - val_acc: 0.8303\n",
      "Epoch 10/30\n",
      "14400/14400 [==============================] - 4s 268us/step - loss: 0.2452 - acc: 0.9126 - val_loss: 0.3952 - val_acc: 0.8247\n",
      "Epoch 11/30\n",
      "14400/14400 [==============================] - 4s 244us/step - loss: 0.2161 - acc: 0.9298 - val_loss: 0.3699 - val_acc: 0.8400\n",
      "Epoch 12/30\n",
      "14400/14400 [==============================] - 4s 288us/step - loss: 0.1897 - acc: 0.9417 - val_loss: 0.3393 - val_acc: 0.8572\n",
      "Epoch 13/30\n",
      "14400/14400 [==============================] - 4s 268us/step - loss: 0.1699 - acc: 0.9515 - val_loss: 0.3311 - val_acc: 0.8639\n",
      "Epoch 14/30\n",
      "14400/14400 [==============================] - 4s 253us/step - loss: 0.1531 - acc: 0.9573 - val_loss: 0.3396 - val_acc: 0.8533\n",
      "Epoch 15/30\n",
      "14400/14400 [==============================] - 4s 252us/step - loss: 0.1300 - acc: 0.9689 - val_loss: 0.3590 - val_acc: 0.8428\n",
      "Epoch 16/30\n",
      "14400/14400 [==============================] - 4s 248us/step - loss: 0.1161 - acc: 0.9748 - val_loss: 0.3337 - val_acc: 0.8683\n",
      "Epoch 17/30\n",
      "14400/14400 [==============================] - 4s 259us/step - loss: 0.1145 - acc: 0.9714 - val_loss: 0.3336 - val_acc: 0.8667\n",
      "Epoch 18/30\n",
      "14400/14400 [==============================] - 4s 263us/step - loss: 0.0935 - acc: 0.9823 - val_loss: 0.4207 - val_acc: 0.8239\n",
      "Epoch 19/30\n",
      "14400/14400 [==============================] - 4s 297us/step - loss: 0.0804 - acc: 0.9867 - val_loss: 0.3792 - val_acc: 0.8414\n",
      "Epoch 20/30\n",
      "14400/14400 [==============================] - 5s 322us/step - loss: 0.0698 - acc: 0.9903 - val_loss: 0.3720 - val_acc: 0.8439\n",
      "Epoch 21/30\n",
      "14400/14400 [==============================] - 4s 258us/step - loss: 0.0577 - acc: 0.9950 - val_loss: 0.4426 - val_acc: 0.8219\n",
      "Epoch 22/30\n",
      "14400/14400 [==============================] - 4s 257us/step - loss: 0.0578 - acc: 0.9928 - val_loss: 0.4134 - val_acc: 0.8342\n",
      "Epoch 23/30\n",
      "14400/14400 [==============================] - 4s 304us/step - loss: 0.0516 - acc: 0.9933 - val_loss: 0.3572 - val_acc: 0.8603\n",
      "Epoch 24/30\n",
      "14400/14400 [==============================] - 5s 351us/step - loss: 0.0442 - acc: 0.9960 - val_loss: 0.4810 - val_acc: 0.8114\n",
      "Epoch 25/30\n",
      "14400/14400 [==============================] - 5s 348us/step - loss: 0.0409 - acc: 0.9970 - val_loss: 0.3510 - val_acc: 0.8669\n",
      "Epoch 26/30\n",
      "14400/14400 [==============================] - 4s 287us/step - loss: 0.0375 - acc: 0.9965 - val_loss: 0.4153 - val_acc: 0.8433\n",
      "Epoch 27/30\n",
      "14400/14400 [==============================] - 4s 257us/step - loss: 0.0310 - acc: 0.9984 - val_loss: 0.4274 - val_acc: 0.8383\n",
      "Epoch 28/30\n",
      "14400/14400 [==============================] - 3s 228us/step - loss: 0.0295 - acc: 0.9984 - val_loss: 0.4439 - val_acc: 0.8333\n",
      "Epoch 29/30\n",
      "14400/14400 [==============================] - 3s 228us/step - loss: 0.0282 - acc: 0.9985 - val_loss: 0.4691 - val_acc: 0.8211\n",
      "Epoch 30/30\n",
      "14400/14400 [==============================] - 3s 234us/step - loss: 0.0253 - acc: 0.9989 - val_loss: 0.3802 - val_acc: 0.8567\n"
     ]
    }
   ],
   "source": [
    "# HyperParameters whose values can be and in fact are \n",
    "# altered for obtaining different accuracies in results.\n",
    "\n",
    "validation_data_split = 0.2\n",
    "num_epochs = 30\n",
    "model_batch_size = 512\n",
    "tb_batch_size = 128\n",
    "early_patience = 100\n",
    "\n",
    "tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "# Read Dataset\n",
    "dataset = pd.read_csv('GSC_Vivek_RawDatasetTake.csv')\n",
    "#/Users/vivad/PycharmProjects/ML_Project2/GSC_Vivek_RawDatasetTake.csv\n",
    "# Process Dataset\n",
    "processedData, processedLabel = processData(dataset)\n",
    "history = model.fit(processedData\n",
    "                    , processedLabel\n",
    "                    , validation_split=validation_data_split\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeLabel(encodedLabel):\n",
    "    \n",
    "    # Converting the encoded target labels into their original string form\n",
    "    # such as Fizz, Buzz, etc\n",
    "    \n",
    "    if encodedLabel == 0:\n",
    "        return \"0\"\n",
    "    elif encodedLabel == 1:\n",
    "        return \"1\"\n",
    "    \n",
    "    else:\n",
    "        print(\"DecodeLabelError\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1025)\n",
      "\n",
      "\n",
      " By Vivek Adithya Srinivasa Raghavan\n",
      " UB#: 50290569 \n",
      " Neural Networks based Regression of Author Handwriting matches\n",
      " Using 1 Input layer, 1 output, and 1 hidden layer, with ReLU\n",
      " Errors: 370  Correct :1630\n",
      " Accuracy: 81.5\n"
     ]
    }
   ],
   "source": [
    "wrong   = 0\n",
    "right   = 0\n",
    "\n",
    "#testData = pd.read_csv('testing.csv')\n",
    "testData = processData(dataset, 2)\n",
    "\n",
    "#.as_matrix()\n",
    "processedTestData = testData.iloc[:,:1024].as_matrix()\n",
    "processedTestLabel1 = testData.iloc[:,1024]\n",
    "\n",
    "\n",
    "print(testData.shape)\n",
    "print(\"\\n\")\n",
    "#processedTestData  = encodeData(testData['input'].values)\n",
    "processedTestLabel = encodeLabel(processedTestLabel1)\n",
    "predictedTestLabel = []\n",
    "\n",
    "for i,j in zip(processedTestData,processedTestLabel):\n",
    "    y = model.predict(np.array(i).reshape(-1,1024))\n",
    "   # print(np.array(i))\n",
    "    predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "    \n",
    "    if (j.argmax() == y.argmax()):\n",
    "        right = right + 1\n",
    "        #print(\"Jarg \"+str(j.argmax()))\n",
    "        #print(str(y.argmax()))\n",
    "    else:\n",
    "        wrong = wrong + 1\n",
    "        #print(\"Jarg \"+str(j.argmax()))\n",
    "        #print(str(y.argmax()))\n",
    "\n",
    "\n",
    "\n",
    "print(\" By Vivek Adithya Srinivasa Raghavan\")\n",
    "print(\" UB#: 50290569 \")\n",
    "\n",
    "\n",
    "print(\" Neural Networks based Regression of Author Handwriting matches\")\n",
    "print(\" Using 1 Input layer, 1 output, and 1 hidden layer, with ReLU\")\n",
    "\n",
    "\n",
    "\n",
    "print(\" Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "print(\" Accuracy: \" + str(right/(right+wrong)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
